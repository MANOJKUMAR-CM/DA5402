{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7c1fdbf-38b9-4fc4-8329-6e3cf7c4cd30",
   "metadata": {},
   "source": [
    "## Problem:  Given a dataset of bike trips containing the location with geo-spatial coordinates, compute the total distance commuted by the users collectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39042d46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T09:01:38.177062Z",
     "start_time": "2024-01-30T09:01:37.960412Z"
    }
   },
   "source": [
    "#### The dataset is taken from https://github.com/danielbeach/data-engineering-practice/tree/main/Exercises/Exercise-6/data\n",
    "\n",
    "#### It is possible to compute the distance between two geo-spatial coordinates (lat-long pair). \n",
    "Refer https://www.movable-type.co.uk/scripts/latlong.html for the formula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df95551c",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be409e6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T10:00:01.042352Z",
     "start_time": "2024-01-30T10:00:00.437890Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9d47e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T10:01:11.191039Z",
     "start_time": "2024-01-30T10:01:11.185321Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add progress bar to pandas apply() functions\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b66204",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T11:24:55.733910Z",
     "start_time": "2024-01-30T11:24:55.724910Z"
    }
   },
   "outputs": [],
   "source": [
    "# compute the 'haversine' distance in meters between two geo positions\n",
    "# Refer https://www.movable-type.co.uk/scripts/latlong.html for the formula.\n",
    "def distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371e3; # radius of Earth in metres\n",
    "    φ1 = lat1 * math.pi/180; # φ, λ in radians\n",
    "    φ2 = lat2 * math.pi/180;\n",
    "    Δφ = (lat2-lat1) * math.pi/180;\n",
    "    Δλ = (lon2-lon1) * math.pi/180;\n",
    "\n",
    "    a = math.sin(Δφ/2) * math.sin(Δφ/2) + math.cos(φ1) * math.cos(φ2) * math.sin(Δλ/2) * math.sin(Δλ/2);\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a));\n",
    "\n",
    "    d = R * c; # distance in metres\n",
    "    return math.nan if math.isnan(d) else int(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f45d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T09:58:56.476334Z",
     "start_time": "2024-01-30T09:58:56.467968Z"
    }
   },
   "source": [
    "#### load the data file into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5027b52d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T10:25:59.729198Z",
     "start_time": "2024-01-30T10:25:59.723236Z"
    }
   },
   "outputs": [],
   "source": [
    "DATAFILE = 'Divvy_Trips_2020_Q1.xlsx'\n",
    "DATAFILE_PQ = 'Divvy_Trips_2020_Q1.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e84e990",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T13:06:49.048231Z",
     "start_time": "2024-01-30T13:06:46.895722Z"
    }
   },
   "outputs": [],
   "source": [
    "#df = pd.read_excel(DATAFILE)\n",
    "#df = df.dropna()\n",
    "#df = df.astype(str)\n",
    "#df.to_parquet(DATAFILE_PQ)\n",
    "\n",
    "# load the data file into a pandas dataframe\n",
    "df = pd.read_parquet(DATAFILE_PQ)\n",
    "# get rid of the empty rows.\n",
    "df = df.dropna()\n",
    "# view the top 5 records.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea42148",
   "metadata": {},
   "source": [
    "### collect the geo location pairs per record and call the distance function on each record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97643fbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T13:07:17.043143Z",
     "start_time": "2024-01-30T13:07:07.018813Z"
    }
   },
   "outputs": [],
   "source": [
    "df['distance'] = df[['start_lat', 'start_lng', 'end_lat', 'end_lng']].progress_apply(\n",
    "    lambda x: distance(float(x[0]), float(x[1]), float(x[2]), float(x[3])), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dcb768",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T10:43:53.843685Z",
     "start_time": "2024-01-30T10:43:52.932538Z"
    }
   },
   "source": [
    "### now compute the total by invoking the sum method of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd53c13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T13:07:19.925293Z",
     "start_time": "2024-01-30T13:07:19.917966Z"
    }
   },
   "outputs": [],
   "source": [
    "total = df.distance.sum()\n",
    "print('Total trip distance is', int(total/1000), 'kilometers over', df.shape[0], 'trips')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c68c98",
   "metadata": {},
   "source": [
    "### let's look at some of the long distance trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d631e746",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T13:07:24.416526Z",
     "start_time": "2024-01-30T13:07:24.390105Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the records with more than 20km trips\n",
    "df[df.distance>20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a737abd4",
   "metadata": {},
   "source": [
    "## Let's solve it using PySpark now, hopefully using parallel processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbd1e53",
   "metadata": {},
   "source": [
    "### import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e7787",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T10:51:42.869673Z",
     "start_time": "2024-01-30T10:51:42.863361Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255f3d21",
   "metadata": {},
   "source": [
    "### create the spark context, which will create the spark backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69875740",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T10:54:53.368209Z",
     "start_time": "2024-01-30T10:54:50.012320Z"
    }
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d26b173",
   "metadata": {},
   "source": [
    "#### We can monitor the operation via http://localhost:4040\n",
    "### let's create Spark Dataframe from the pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9358a108",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T13:08:24.747207Z",
     "start_time": "2024-01-30T13:07:29.184479Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    " \n",
    "# Building the SparkSession and name\n",
    "# it :'pandas to spark'\n",
    "spark = SparkSession.builder.appName(\"pandas to spark\").getOrCreate()\n",
    " \n",
    "# create DataFrame\n",
    "df_spark = spark.createDataFrame(df)\n",
    " \n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a850c71",
   "metadata": {},
   "source": [
    "### let's compute the distance from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3028e0da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T14:11:23.602809Z",
     "start_time": "2024-01-30T14:11:23.595657Z"
    }
   },
   "outputs": [],
   "source": [
    "dist = df_spark.rdd.map(lambda x: distance(float(x[8]), float(x[9]), float(x[10]), float(x[11])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d66a58",
   "metadata": {},
   "source": [
    "### let's now compute the total distance by reducing the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b0e42d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T14:11:30.010358Z",
     "start_time": "2024-01-30T14:11:26.090919Z"
    }
   },
   "outputs": [],
   "source": [
    "total_distance = dist.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fea0635",
   "metadata": {},
   "source": [
    "### report the findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e76ae9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T14:11:34.348534Z",
     "start_time": "2024-01-30T14:11:33.076658Z"
    }
   },
   "outputs": [],
   "source": [
    "count = df_spark.count()\n",
    "print('Total trip distance is', int(total_distance/1000), 'kilometers over', count, 'trips')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa9357f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T11:26:47.955620Z",
     "start_time": "2024-01-30T11:26:44.605220Z"
    }
   },
   "source": [
    "### Let's try another way of doing this in Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db900a2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T12:40:46.901223Z",
     "start_time": "2024-01-30T12:40:43.312343Z"
    }
   },
   "source": [
    "#### let's get the lat lon values from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab556a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T13:12:50.121397Z",
     "start_time": "2024-01-30T13:12:50.096207Z"
    }
   },
   "outputs": [],
   "source": [
    "latlon_records = df[['start_lat', 'start_lng', 'end_lat', 'end_lng']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02330165",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T13:12:52.048301Z",
     "start_time": "2024-01-30T13:12:52.040725Z"
    }
   },
   "outputs": [],
   "source": [
    "latlon_records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0c04c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T12:35:56.274660Z",
     "start_time": "2024-01-30T12:35:54.933524Z"
    }
   },
   "source": [
    "### let's convert the data in to a RDD.  Here the number of slices is an important parameter that controls the number of jobs that are runnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4b38de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T14:07:16.598659Z",
     "start_time": "2024-01-30T14:07:12.028631Z"
    }
   },
   "outputs": [],
   "source": [
    "latlon_rdd = sc.parallelize(latlon_records, numSlices=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1895d1bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T12:40:25.164202Z",
     "start_time": "2024-01-30T12:40:25.157170Z"
    }
   },
   "source": [
    "### let's now run the same job of computing the individual distances followed by the total distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7330c085",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T14:09:33.531788Z",
     "start_time": "2024-01-30T14:09:29.716022Z"
    }
   },
   "outputs": [],
   "source": [
    "total_distance = latlon_rdd \\\n",
    ".map(lambda x: distance(float(x[0]), float(x[1]), float(x[2]), float(x[3]))) \\\n",
    ".reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b4b583",
   "metadata": {},
   "source": [
    "### report the findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b4843d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T14:10:18.601333Z",
     "start_time": "2024-01-30T14:10:15.430165Z"
    }
   },
   "outputs": [],
   "source": [
    "count = latlon_rdd.count()\n",
    "print('Total trip distance is', int(total_distance/1000), 'kilometers over', count, 'trips')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267b052a",
   "metadata": {},
   "source": [
    "### Let's look at another example of parallel processing files using Spark\n",
    "\n",
    "## Problem: Given a folder of images, OCR them and compute the token distribution\n",
    "\n",
    "* Convert the image to text\n",
    "* combine the texts into a large blob\n",
    "* tokenize the text into token seperated by whitespaces\n",
    "* compute the number of unique tokens with their respect counts\n",
    "* save the output in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a86f70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T14:29:54.394168Z",
     "start_time": "2024-01-30T14:29:54.388142Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "FOLDER = 'funsd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c4cce3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T14:32:53.819729Z",
     "start_time": "2024-01-30T14:32:53.809414Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_files = list(map(lambda x: FOLDER + '/' + x.name, Path(FOLDER).glob('*.*')))\n",
    "list_of_files[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bcf27d",
   "metadata": {},
   "source": [
    "### create a function to invoke the tesseract command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af333b00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T06:34:35.104413Z",
     "start_time": "2024-01-31T06:34:35.096426Z"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess as sp\n",
    "import os\n",
    "my_env = os.environ.copy()\n",
    "my_env[\"OMP_THREAD_LIMIT\"] = '1'\n",
    "\n",
    "def ocr_task(path):\n",
    "    # invoke the tesseract command to run OCR on the input image\n",
    "    # set the output to go to stdout so that we can collect it in memory.\n",
    "    result = sp.run(['tesseract', path, '-'], \n",
    "                     stdout=sp.PIPE, stderr=sp.PIPE, \n",
    "                     check=True, text=True,\n",
    "                     env=my_env)\n",
    "    # check if the command executed without errors\n",
    "    if result.returncode == 0:\n",
    "        # return the OCR text\n",
    "        return result.stdout\n",
    "    # return blank to filter later.\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61356682",
   "metadata": {},
   "source": [
    "### check if the function is working fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d0fa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T06:34:37.274901Z",
     "start_time": "2024-01-31T06:34:36.541667Z"
    }
   },
   "outputs": [],
   "source": [
    "ocr_task('funsd/0060308251.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962b1b57",
   "metadata": {},
   "source": [
    "### let's gauge the time taken to run the OCR task in sequential order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12dfadc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T06:35:56.074347Z",
     "start_time": "2024-01-31T06:35:56.069117Z"
    }
   },
   "outputs": [],
   "source": [
    "text_fragments = map(ocr_task, list_of_files[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0bb0af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T06:37:53.073829Z",
     "start_time": "2024-01-31T06:35:57.049250Z"
    }
   },
   "outputs": [],
   "source": [
    "all_text = \"\\n\".join(text_fragments)\n",
    "all_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23d6839",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T05:40:05.861884Z",
     "start_time": "2024-01-31T05:40:05.854947Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aaee7747",
   "metadata": {},
   "source": [
    "### let's try to parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbb43f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T06:35:00.866659Z",
     "start_time": "2024-01-31T06:35:00.855201Z"
    }
   },
   "outputs": [],
   "source": [
    "lof_rdd = sc.parallelize(list_of_files[::-1], numSlices=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd33da8",
   "metadata": {},
   "source": [
    "### we will configure the ocr_task as the mapper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfdca9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T06:35:03.052045Z",
     "start_time": "2024-01-31T06:35:03.045816Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = lof_rdd.map(ocr_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e86bda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T15:01:08.840720Z",
     "start_time": "2024-01-30T15:01:08.826622Z"
    }
   },
   "source": [
    "### we will now tokenize each of the texts into an array of tokens\n",
    "#### we use flatMap here which is an equivalent of map() followed by flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bf4781",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T06:35:04.065726Z",
     "start_time": "2024-01-31T06:35:04.059829Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# flatmap gets one dimensional array, while map gets an array of array.\n",
    "# as we are interested in counting the unique tokens, we need a flattened array.\n",
    "tokens = texts.flatMap(lambda x: re.findall(r'[A-Za-z\\']+', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b084ab1",
   "metadata": {},
   "source": [
    "### let's convert every token to a tuple (token,1), which we can reduce by key later to get the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ab1f49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T06:35:05.581104Z",
     "start_time": "2024-01-31T06:35:05.574754Z"
    }
   },
   "outputs": [],
   "source": [
    "token_tuples = tokens.map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b5a287",
   "metadata": {},
   "source": [
    "### let's count by key to get the distribution now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee825210",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T06:35:44.836550Z",
     "start_time": "2024-01-31T06:35:07.122925Z"
    }
   },
   "outputs": [],
   "source": [
    "token_counts = token_tuples.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed5f09b",
   "metadata": {},
   "source": [
    "### Let's create a dataframe with the estimated token distribution results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc9602",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T05:47:00.561397Z",
     "start_time": "2024-01-31T05:47:00.550678Z"
    }
   },
   "outputs": [],
   "source": [
    "newdf = pd.DataFrame({\"tokens\":list(token_counts.keys()), \"freq\":list(token_counts.values())})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c007f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T05:46:58.711031Z",
     "start_time": "2024-01-31T05:46:58.692501Z"
    }
   },
   "outputs": [],
   "source": [
    "newdf.sort_values('freq', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666d759d",
   "metadata": {},
   "source": [
    "### now, save it as a spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481d9bec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T05:48:11.356141Z",
     "start_time": "2024-01-31T05:48:09.472933Z"
    }
   },
   "outputs": [],
   "source": [
    "newdf.to_excel('/tmp/output.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e7d86f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fadebe9f",
   "metadata": {},
   "source": [
    "## Let's process Amazon reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d624572",
   "metadata": {},
   "source": [
    "### How to do complex transformation using map functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa148fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to read a text file\n",
    "def read_file(path):\n",
    "    with open(path, 'r') as file:\n",
    "        textdata = file.read()\n",
    "        file.close()\n",
    "        return textdata\n",
    "    \n",
    "# create a helper function to view a sample of a text file\n",
    "def view_file(path, length=50, lines=False):\n",
    "    textdata = read_file(path)\n",
    "\n",
    "    # if we need lines, split it and display the required number of lines.\n",
    "    sample = \"\\n\".join(textdata.split(\"\\n\")[:length]) if lines else textdata[:length]\n",
    "        \n",
    "    print(\"TextSize:\", len(textdata), \"\\n\\nSample:\", sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23d0e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFILE = 'Gourmet_Foods.txt'\n",
    "view_file(DATAFILE, 20, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274af8e7",
   "metadata": {},
   "source": [
    "### data file is a single archive of reviews.  We need to extract the review/text to construct a dataset for further processing\n",
    "* scan the file for \"review/text:\" pattern and extract the right side of the pattern.\n",
    "* also get the product id, so that we can map the review text to the product id.\n",
    "* let's also pick up the review/score to record the star rating.\n",
    "* now we should have a triplet with (productid, rating, review_text)\n",
    "* if we carefully see, the reviews are seperated by multiple consecutive newlines!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373d6087",
   "metadata": {},
   "source": [
    "### let's read the data and split the data based on consecutive newlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e989f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# read the data file and split by \\n\\n+\n",
    "reviews = re.split(r'\\n\\n+', read_file(DATAFILE))\n",
    "print(\"number of reviews:\", len(reviews))\n",
    "print(reviews[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b885ef9a",
   "metadata": {},
   "source": [
    "### create the parallelizable dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac3ee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_rdd = sc.parallelize(reviews, numSlices=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ab67a8",
   "metadata": {},
   "source": [
    "### we can define a function to process each block to extract the triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795b65c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(text):\n",
    "    match = re.search('product/productId: (.+)', text)\n",
    "    product_id = match.group(1) if match else \"\"\n",
    "    match = re.search('review/score: (.+)', text)\n",
    "    star_rating = float(match.group(1)) if match else 0.0\n",
    "    match = re.search('review/text: (.+)', text)\n",
    "    review_text = match.group(1) if match else \"\"\n",
    "    return (product_id, star_rating, review_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ade604",
   "metadata": {},
   "source": [
    "### let's extend the beam to include the extraction of triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebae032",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = reviews_rdd.map(process).filter(lambda x: x[0] != \"\" and x[1]>0.0 and x[2] != \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ee5d18",
   "metadata": {},
   "source": [
    "### As a task, let's group the data by product id to find the average rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054c4ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get rid of the texts first and them group the data by key (product id)\n",
    "product_rating = triplets.map(lambda x: (x[0], x[1])).groupByKey().map(lambda p_r: (p_r[0], round(sum(p_r[1])/len(p_r[1]),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2ae635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the pipeline now.\n",
    "result = product_rating.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54e31e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the result by ratings\n",
    "result_sorted = sorted(result, key=lambda tup: tup[1], reverse=True)\n",
    "result_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f60e349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
