{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## packages required : selenium\n",
    "# !pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook is used for debugging and initial creation of code**\n",
    "\n",
    "**Mainly created to play around with and all code in this NB has been converted to separate python modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import hashlib\n",
    "\n",
    "import re\n",
    "from PIL import Image\n",
    "from bson.binary import Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from bson.binary import Binary\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the webdriver\n",
    "driver = webdriver.Chrome()  # Make sure you have ChromeDriver installed\n",
    "\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless=new\")\n",
    "# chrome_options.add_argument(\"--force-device-scale-factor=1\")  # Prevent DPI scaling issues\n",
    "# chrome_options.add_argument(\"--window-size=1920,1080\")  # Mandatory for consistent rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading the page\n",
    "url = \"https://news.google.com/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulating scrolling for webpage and load-more button\n",
    "\n",
    "# To check whether the loading has finished or not\n",
    "def is_page_loaded(driver):\n",
    "    return driver.execute_script(\"return document.readyState\") == \"complete\"\n",
    "\n",
    "# getting the last scroll height\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "# maximum scroll iterations allowed is 10\n",
    "scroll_iterations = 0\n",
    "\n",
    "while scroll_iterations < 5:\n",
    "\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "    # Loop to check loading status and we will wait for maximum of 3 seconds to load the page\n",
    "    timeout = 3  # seconds\n",
    "    start_time = time.time()\n",
    "\n",
    "    while not is_page_loaded(driver):\n",
    "        if time.time() - start_time > timeout:\n",
    "            print(\"Timeout reached. The page is still loading.\")\n",
    "            break\n",
    "\n",
    "    # checking for load more button as well\n",
    "    try:\n",
    "        load_more_button = driver.find_element(By.CLASS_NAME, \"load-more-button\")\n",
    "        load_more_button.click()\n",
    "        time.sleep(2)\n",
    "    except : pass\n",
    "\n",
    "    # getting the new height\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    # if the heights are same and the page is loaded , then we do timeout\n",
    "    if (new_height == last_height) and is_page_loaded(driver) :\n",
    "        break\n",
    "\n",
    "    # updating the height and scroll_iterations\n",
    "    last_height = new_height\n",
    "    scroll_iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now extracting the contents using beautifulsoup\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## putting everything inside a fn\n",
    "\n",
    "def module_1_web_scrapping_with_lazy_loading(driver = driver, url = \"https://news.google.com/\"):\n",
    "    \n",
    "    # getting the url\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "    # time.sleep(2)\n",
    "\n",
    "    # simulating scrolling for webpage and load-more button\n",
    "    # getting the last scroll height\n",
    "    old_items = len(driver.find_elements(By.CSS_SELECTOR, \".item-class, div, article, li\"))\n",
    "    last_height = driver.execute_script(\n",
    "        \"return Math.max(document.documentElement.scrollHeight, document.body.scrollHeight, document.documentElement.clientHeight);\")\n",
    "\n",
    "    # maximum scroll iterations allowed is 10\n",
    "    scroll_iterations = 0\n",
    "\n",
    "    while scroll_iterations < 5:\n",
    "\n",
    "        driver.execute_script(\"\"\"window.scrollBy(0, Math.max(document.documentElement.scrollHeight, \n",
    "                              document.body.scrollHeight, document.documentElement.clientHeight));\"\"\")\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Loop to check loading status and we will wait for maximum of 5 seconds to load the page\n",
    "        timeout = 5  # seconds\n",
    "        start_time = time.time()\n",
    "\n",
    "        while not is_page_loaded(driver):\n",
    "            if time.time() - start_time > timeout:\n",
    "                print(\"Timeout reached. The page is still loading.\")\n",
    "                break\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        # checking for load more button as well\n",
    "        try:\n",
    "            load_more_button = driver.find_element(By.CLASS_NAME, \"load-more-button\")\n",
    "            load_more_button.click()\n",
    "            time.sleep(2)\n",
    "        except : pass\n",
    "\n",
    "        # getting the new height and new items\n",
    "        new_items = len(driver.find_elements(By.CSS_SELECTOR, \".item-class, div, article, li\"))\n",
    "        new_height = driver.execute_script(\n",
    "        \"return Math.max(document.documentElement.scrollHeight, document.body.scrollHeight, document.documentElement.clientHeight);\")\n",
    "\n",
    "        # print(last_height, new_height, old_items, new_items)\n",
    "\n",
    "        # if the heights are same and the page is loaded , then we do timeout\n",
    "        if (new_height == last_height) and is_page_loaded(driver) and (new_items == old_items):\n",
    "            break\n",
    "\n",
    "        # updating the height and scroll_iterations\n",
    "        last_height = new_height\n",
    "        old_items = new_items\n",
    "        scroll_iterations += 1\n",
    "    \n",
    "    ## Now extracting the contents using beautifulsoup\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "page_content_module_1 = module_1_web_scrapping_with_lazy_loading()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the top stories\n",
    "\"\"\" Doubt to be discussed with achyutha (whether it is just stories or top stories)\"\"\"\n",
    "story_links = soup.find_all('a', href=re.compile(r'^./stories/')) \n",
    "\n",
    "# for saving the links\n",
    "top_story_links = []\n",
    "\n",
    "# getting all the links\n",
    "for links in story_links:\n",
    "    story_url = f\"https://news.google.com{links['href'].lstrip('.')}\"\n",
    "    # print(story_url)\n",
    "    top_story_links.append(story_url)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of stories link extracted : 29\n"
     ]
    }
   ],
   "source": [
    "def module_2_stories_link(soup, section = 'stories'):\n",
    "\n",
    "    story_links = soup.find_all('a', href=re.compile(f'^./{section}/')) \n",
    "\n",
    "    # for saving the links\n",
    "    top_story_links = []\n",
    "\n",
    "    # getting all the links\n",
    "    for links in story_links:\n",
    "        story_url = f\"https://news.google.com{links['href'].lstrip('.')}\"\n",
    "        # print(story_url)\n",
    "        top_story_links.append(story_url)\n",
    "    \n",
    "    print(f\"Total number of stories link extracted : {len(top_story_links)}\")\n",
    "\n",
    "    return top_story_links\n",
    "\n",
    "m2_top_story_links = module_2_stories_link(soup = page_content_module_1, section = 'stories')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the thumbnail and links from the page\n",
    "m3_soup1 = module_1_web_scrapping_with_lazy_loading(driver = driver,\n",
    "                                                    url = m2_top_story_links[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The structure of the articles in the page should be:\n",
    "\n",
    "article (class : \"MQsxIb xTewfe tXImLc R7GTQ keNKEd keNKEd VkAdve GU7x0c JMJvke q4atFc\")\n",
    "    |\n",
    "    |\n",
    "    ------- h4 (class : \"ipQwMb ekueJc RD0gLb\")\n",
    "    |\n",
    "    |\n",
    "    ------- img (class : \"tvs3Id QwxBBf\")\n",
    "\n",
    "    \n",
    "From this we will extract the image and thumbnail\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "Data extraction Loop :\n",
    "\n",
    "-> Get the top stories link\n",
    "\n",
    "-> For each link:\n",
    "    -> extract the page\n",
    "    -> for each article in the page:\n",
    "        -> extract image and thumbnail\n",
    "\n",
    "##############################################################################################        \n",
    "\"\"\"\n",
    "\n",
    "all_news_cards = m3_soup1.find_all('article', {'class': 'MQsxIb xTewfe tXImLc R7GTQ keNKEd keNKEd VkAdve GU7x0c JMJvke q4atFc'})\n",
    "news_stories = []\n",
    "\n",
    "for news_articles in all_news_cards:\n",
    "        \n",
    "        # Extract headline\n",
    "        headline = news_articles.find('h4', {'class': 'ipQwMb ekueJc RD0gLb'})\n",
    "        headline_text = headline.get_text(strip=True) if headline else \"None\"\n",
    "        \n",
    "        # Extracting thumbnails (using src or data-src attr)\n",
    "        img = news_articles.find('img', {'class': 'tvs3Id QwxBBf'})\n",
    "        # print(img)\n",
    "\n",
    "        thumbnail_url = None\n",
    "        if img:\n",
    "            thumbnail_url = img.get('src') or img.get('data-src')\n",
    "            if thumbnail_url or thumbnail_url.startswith('//'):\n",
    "                thumbnail_url = f'https://news.google.com{thumbnail_url}'\n",
    "        \n",
    "        news_stories.append({\n",
    "            'headline': headline_text,\n",
    "            'thumbnail': thumbnail_url\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'headline': 'Blackhawk pilot killed in DC plane crash identified while trans soldier receives flak on social media: Wh',\n",
       " 'thumbnail': 'https://news.google.com/api/attachments/CC8iI0NnNDRTbVpWVEZGdWFpMW5ibGRWVFJDZkF4ampCU2dLTWdB=-w100-h100-p-df-rw'}"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_stories[0] # sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages available to scrap : 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e794d0ff71934aa8bea2277c614c950a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Full loop code inside a fn\n",
    "\n",
    "def module_3_thumbnail_img_extraction(driver = driver,\n",
    "                                      top_stories_url = m2_top_story_links[:3],\n",
    "                                      article_class_name = \"MQsxIb xTewfe tXImLc R7GTQ keNKEd keNKEd VkAdve GU7x0c JMJvke q4atFc\",\n",
    "                                      headline_class_name = \"ipQwMb ekueJc RD0gLb\",\n",
    "                                      img_class_name = \"tvs3Id QwxBBf\"):\n",
    "    \n",
    "    thumbnail_img_list = []\n",
    "    print(f\"Total pages available to scrap : {len(top_stories_url)}\")\n",
    "\n",
    "    for url in tqdm(top_stories_url):\n",
    "\n",
    "        # extracting the contents of all pages \n",
    "        top_story_page_content = module_1_web_scrapping_with_lazy_loading(driver = driver,\n",
    "                                                    url = url)\n",
    "        \n",
    "        all_news_cards = top_story_page_content.find_all('article', {'class': article_class_name})\n",
    "\n",
    "        for news_articles in all_news_cards:\n",
    "                \n",
    "                # Extract headline\n",
    "                headline = news_articles.find('h4', {'class': headline_class_name})\n",
    "                headline_text = headline.get_text(strip=True) if headline else \"None\"\n",
    "                \n",
    "                # Extracting thumbnails (using src or data-src attr)\n",
    "                img = news_articles.find('img', {'class': img_class_name})\n",
    "                # print(img)\n",
    "\n",
    "                thumbnail_url = None\n",
    "                if img:\n",
    "                    thumbnail_url = img.get('src') or img.get('data-src')\n",
    "                    if thumbnail_url or thumbnail_url.startswith('//'):\n",
    "                        thumbnail_url = f'https://news.google.com{thumbnail_url}'\n",
    "                \n",
    "                thumbnail_img_list.append({\n",
    "                    'headline': headline_text,\n",
    "                    'thumbnail': thumbnail_url\n",
    "                })\n",
    "    \n",
    "    return thumbnail_img_list\n",
    "\n",
    "m3_thumb_img_test = module_3_thumbnail_img_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total datapoints extracted : 166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'headline': 'India’s budget gives tax relief to middle class to boost spending, growth',\n",
       " 'thumbnail': 'https://news.google.com/api/attachments/CC8iK0NnNTNkMVJpTldoV1ZGbDBNWFZpVFJEZ0F4aUFCU2dLTWdhWlFKZ1NwUWM=-w100-h100-p-df-rw'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"total datapoints extracted : {len(m3_thumb_img_test)}\")\n",
    "m3_thumb_img_test[0] # example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25d6efc2b1e416d8e98817b7dade0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## downloading and saving all images\n",
    "\"\"\"\n",
    "root_img - root path (folder) where all images are stored\n",
    "\n",
    "\"\"\"\n",
    "import requests\n",
    "root_img = \"images\"\n",
    "\n",
    "def download_and_store_image(heading_url_data, root_img = \"images\"):\n",
    "\n",
    "    thumbnail_heading_dataset = []\n",
    "    base_val = int(os.listdir(root_img)[-1].split('.')[0].split('_')[-1]) if len(os.listdir(root_img)) > 0 else 0\n",
    "\n",
    "    for i,data_point in tqdm(enumerate(heading_url_data)):\n",
    "\n",
    "        # thumbnail and headline\n",
    "        image_url = data_point['thumbnail']\n",
    "        image_headline = data_point['headline']\n",
    "        image_id = (base_val + i + 1) # (this will be the name of the image as well)\n",
    "\n",
    "        # getting the image and storing them\n",
    "        img_data = requests.get(image_url).content\n",
    "        with open(f\"{root_img}/image_{image_id}.jpg\", 'wb') as handler:\n",
    "            handler.write(img_data)\n",
    "        \n",
    "        # creating a dict to store the data\n",
    "        dp = {\"headline\" : image_headline, \"image_id\" : image_id, \"image_url\" : image_url}\n",
    "\n",
    "        # adding things to the dataset\n",
    "        thumbnail_heading_dataset.append(dp)\n",
    "\n",
    "    return thumbnail_heading_dataset\n",
    "\n",
    "downloaded_headline_images_data = download_and_store_image(heading_url_data = m3_thumb_img_test)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 4 and Module 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My Approach for Module 4 and Module 5**\n",
    "\n",
    "-> for this we need to **hash** the image using `hashlib` package in python\n",
    "\n",
    "-> <u>*Table 1 (headline table) :*</u> contains \n",
    "* headline, \n",
    "* image url, \n",
    "* image id, \n",
    "* image_index (row index of the image in image Table), \n",
    "* image hash \n",
    "* (other metadata as well , if required)\n",
    "\n",
    "-> <u>*Table 2 (image table) :*</u> \n",
    "* Has the hash of the image + hash of the headline \n",
    "* bin of the image as well , if required (here I am storing the bin as well as it these are small images)\n",
    "\n",
    "-> <u>*Logic :*</u> \n",
    "* We can get the image from the ***image_id***, which is the name of the image and stored some other folder (from this we can get the image )\n",
    "* **Hash is there to ensure whether the images and headlines are same** , we will get the image and check its hash with the corresponding row in the images table\n",
    "* while storing images and headlines, if we get same has then we wont store it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'image_hash_1'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to MongoDB\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "\n",
    "# Create/access database and collection (table)\n",
    "db = client['image_text_db']\n",
    "\n",
    "# creating img_table and headline_table \n",
    "img_table = db['thumbnail_table'] \n",
    "headline_table = db['headline_table'] \n",
    "\n",
    "# putting the hash table as unique, so we wont allow duplicates as well\n",
    "img_table.create_index([('image_headline_hash', 1)], unique=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating and storing all the images_hash and headlines\n",
    "\n",
    "def module_4_and_5_store_in_database(root_img, \n",
    "                               extracted_data,\n",
    "                               headline_table,\n",
    "                               img_table):\n",
    "\n",
    "    for i,datapoint in tqdm(enumerate(extracted_data)):\n",
    "\n",
    "        current_headline = datapoint['headline']\n",
    "        current_image_id = datapoint['image_id']\n",
    "        current_url = datapoint['image_url']\n",
    "        current_headline_hash = hashlib.sha256(current_headline.encode('utf-8')).hexdigest()\n",
    "\n",
    "        # getting the image id\n",
    "        img_path = f\"{root_img}/image_{current_image_id}.jpg\"\n",
    "\n",
    "        # Reading the image data\n",
    "        with open(img_path, 'rb') as current_img_data:\n",
    "            # creating the hash of the image\n",
    "            current_img_hash = hashlib.sha256(current_img_data.read()).hexdigest()\n",
    "            binary_image = Binary(current_img_data.read())\n",
    "\n",
    "        # Create table row document\n",
    "        headline_document = {\n",
    "            \"headline\": current_headline,\n",
    "            \"image_url\": current_url,\n",
    "            \"image_id\": current_image_id,\n",
    "            \"image_index\": img_table.count_documents({}),\n",
    "        }\n",
    "\n",
    "        img_document = {\"image_headline_hash\" : current_img_hash + current_headline_hash, \n",
    "                        \"image_bin\" : binary_image}\n",
    "        \n",
    "        try:\n",
    "            # Insert into table of headline and images\n",
    "            img_table.insert_one(img_document)\n",
    "            headline_table.insert_one(headline_document)\n",
    "        \n",
    "        except: pass\n",
    "\n",
    "    print(f\"Successfully stored {img_table.count_documents({})} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407299dd1ab042f2b596ab8cc4750648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully stored 117 records\n"
     ]
    }
   ],
   "source": [
    "module_4_and_5_store_in_database(root_img = root_img, \n",
    "                               extracted_data = downloaded_headline_images_data,\n",
    "                               headline_table = headline_table,\n",
    "                               img_table = img_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import m1\n",
    "import m2\n",
    "import m3\n",
    "import m4_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def module_6_orchestrator(url = \"https://news.google.com/\", # for m1\n",
    "                          section = 'stories', # for m2\n",
    "                          article_class_name = \"MQsxIb xTewfe tXImLc R7GTQ keNKEd keNKEd VkAdve GU7x0c JMJvke q4atFc\", # for m3\n",
    "                          headline_class_name = \"ipQwMb ekueJc RD0gLb\", # for m3\n",
    "                          img_class_name = \"tvs3Id QwxBBf\", # for m3\n",
    "                          host = \"mongodb://localhost:27017/\", # for m4\n",
    "                          root_img = \"images\", # for m4 and m3\n",
    "                          ):\n",
    "    \n",
    "    # getting the base google news page \n",
    "    google_news_page_soup = m1.module_1_web_scrapping_with_lazy_loading(url = url)\n",
    "\n",
    "    # extracting the links from the soup\n",
    "    top_story_links = m2.module_2_stories_link(soup = google_news_page_soup, section = section)[:2]\n",
    "\n",
    "    # now extracting stories , thumbnail from that\n",
    "    headline_thumbnail_url_list = m3.module_3_thumbnail_img_extraction(top_stories_url = top_story_links,\n",
    "                                                                      article_class_name = article_class_name,\n",
    "                                                                      headline_class_name = headline_class_name,\n",
    "                                                                      img_class_name = img_class_name)\n",
    "    \n",
    "    print(f\"{len(headline_thumbnail_url_list)} headline and thumbnail urls have been collected\")\n",
    "\n",
    "    # now downloading and saving those images\n",
    "    headline_downloaded_img = m3.download_and_store_image(heading_url_data = headline_thumbnail_url_list,\n",
    "                                                          root_img = root_img)\n",
    "    \n",
    "    print(f\"{len(headline_downloaded_img)} has been downloaded successfully\")\n",
    "\n",
    "    # now storing them to the database\n",
    "    headline_table, img_table = m4_5.connect_database(host = host)\n",
    "    prev_table_size = headline_table.count_documents({})\n",
    "\n",
    "    # storing in database\n",
    "    m4_5.module_4_and_5_store_in_database(root_img = root_img, \n",
    "                                    extracted_data = headline_downloaded_img,\n",
    "                                    headline_table = headline_table, \n",
    "                                    img_table = img_table)\n",
    "    \n",
    "    curr_table_size = headline_table.count_documents({})\n",
    "\n",
    "    print(f\"{curr_table_size - prev_table_size} rows has been populated in database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module_6_orchestrator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://news.google.com/\" # for m1\n",
    "section = 'stories' # for m2\n",
    "article_class_name = \"MQsxIb xTewfe tXImLc R7GTQ keNKEd keNKEd VkAdve GU7x0c JMJvke q4atFc\" # for m3\n",
    "headline_class_name = \"ipQwMb ekueJc RD0gLb\" # for m3\n",
    "img_class_name = \"tvs3Id QwxBBf\" # for m3\n",
    "host = \"mongodb://localhost:27017/\" # for m4\n",
    "root_img = \"images\" # for m4 and m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1499 4682 432 1229\n",
      "4682 4682 1229 1321\n",
      "4682 4682 1321 1321\n",
      "Total number of stories link extracted : 26\n",
      "Total pages available to scrap : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4646 4646 1239 1239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 headline and thumbnail urls have been collected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53it [00:06,  8.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 has been downloaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53it [00:00, 105.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully stored 53 records\n",
      "53 rows has been populated in database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# getting the base google news page \n",
    "google_news_page_soup = m1.module_1_web_scrapping_with_lazy_loading(url = url)\n",
    "\n",
    "# extracting the links from the soup\n",
    "top_story_links = m2.module_2_stories_link(soup = google_news_page_soup, section = section)[:1]\n",
    "\n",
    "# now extracting stories , thumbnail from that\n",
    "headline_thumbnail_url_list = m3.module_3_thumbnail_img_extraction(top_stories_url = top_story_links,\n",
    "                                                                    article_class_name = article_class_name,\n",
    "                                                                    headline_class_name = headline_class_name,\n",
    "                                                                    img_class_name = img_class_name)\n",
    "\n",
    "print(f\"{len(headline_thumbnail_url_list)} headline and thumbnail urls have been collected\")\n",
    "\n",
    "# now downloading and saving those images\n",
    "headline_downloaded_img = m3.download_and_store_image(heading_url_data = headline_thumbnail_url_list,\n",
    "                                                        root_img = root_img)\n",
    "\n",
    "print(f\"{len(headline_downloaded_img)} has been downloaded successfully\")\n",
    "\n",
    "# now storing them to the database\n",
    "headline_table, img_table = m4_5.connect_database(host = host)\n",
    "prev_table_size = headline_table.count_documents({})\n",
    "\n",
    "# storing in database\n",
    "m4_5.module_4_and_5_store_in_database(root_img = root_img, \n",
    "                                extracted_data = headline_downloaded_img,\n",
    "                                headline_table = headline_table, \n",
    "                                img_table = img_table)\n",
    "\n",
    "curr_table_size = headline_table.count_documents({})\n",
    "\n",
    "print(f\"{curr_table_size - prev_table_size} rows has been populated in database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Create and configure logger\n",
    "logging.basicConfig(filename=\"logs_file.log\",\n",
    "                    format='%(asctime)s %(message)s',\n",
    "                    filemode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an object\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Setting the threshold of logger to DEBUG\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Test messages\n",
    "p = 90\n",
    "# logger.debug(f\"Harmless debug Message\")\n",
    "logger.info(f\"Just an information {p}\")\n",
    "# logger.warning(\"Its a Warning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating a YAML file for configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import io\n",
    "\n",
    "# Define data\n",
    "params = {\n",
    "\"url\" : \"https://news.google.com/\", # for m1\n",
    "\"section\" : 'stories', # for m2\n",
    "\"article_class_name\" : \"MQsxIb xTewfe tXImLc R7GTQ keNKEd keNKEd VkAdve GU7x0c JMJvke q4atFc\", # for m3\n",
    "\"headline_class_name\" : \"ipQwMb ekueJc RD0gLb\", # for m3\n",
    "\"img_class_name\" : \"tvs3Id QwxBBf\", # for m3\n",
    "\"host\" : \"mongodb://localhost:27017/\", # for m4\n",
    "\"root_img\" : \"images\", # for m4 and m3\n",
    "}\n",
    "\n",
    "# Write YAML file\n",
    "with io.open('All_Parameters.yaml', 'w', encoding='utf8') as outfile:\n",
    "    yaml.dump(params, outfile, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "# Read YAML file\n",
    "with open('All_Parameters.yaml', 'r') as stream:\n",
    "    data_loaded = yaml.safe_load(stream)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
